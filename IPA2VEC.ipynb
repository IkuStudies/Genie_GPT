{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fgvhHK5IbTceaORtWGUvc6MB_kutVfHU",
      "authorship_tag": "ABX9TyPo7M/jWlEdWgVJ/a8YaziI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IkuStudies/Genie_GPT/blob/main/IPA2VEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "iplBf5Fh3BSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IJR6Ngja288L"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# IPA mapping dictionary\n",
        "ipa_mapping = {\n",
        "    'ʉ': 1, 'ɯ': 1, 'u': 1, 'ʏ': 1, 'ʊ': 1,\n",
        "    'ɵ': 2, 'ɤ': 2, 'o': 2, 'ə': 2, 'ɤ̞': 2,\n",
        "    'e̞': 3, 'ø̞': 3, 'ɛ': 3, 'œ': 3, 'ɶ': 3, 'ɜ': 3, 'ɞ': 3, 'e': 3, 'ø': 3, 'ɘ': 3, 'ɪ': 3, 'i': 3, 'y': 3, 'ɨ': 3,\n",
        "    'ɐ': 4, 'ʌ': 4, 'ʡ': 4, 'ʔ': 4, 'ʔ̞': 4, 'ʡ̆': 4,\n",
        "    'æ': 5, 'a': 5, 'ä': 5,\n",
        "    'o̞': 6, 'ɔ': 6, 'ɑ': 6, 'ɒ': 6,\n",
        "    'm̥': 7, 'm': 7, 'ɱ': 7, 'ɳ̊': 7,\n",
        "    'n̼': 8, 'n̥': 8, 'n': 8, 'ɳ': 8, 'ɲ̊': 8, 'ɲ': 8, 'ŋ̊': 8, 'ŋ': 8, 'ɴ': 8, 'n': 8,\n",
        "    'p': 9, 'b': 9, 'p̪': 9, 'b̪': 9, 'ʙ̥': 9, 'ʙ': 9, 'ɹ̥': 9,\n",
        "    'd̼': 10, 'd': 10, 'ɖ': 10,\n",
        "    't': 11, 'ʈ': 11, 't̼': 11,\n",
        "    'j': 12, 'ɟ': 12,\n",
        "    'k': 13, 'q': 13, 'x': 13, 'χ': 13, 'ħ': 13, 'c': 13,\n",
        "    'ɣ': 14, 'ɡ': 14, 'ɢ': 14, 'ɢ̆': 14,\n",
        "    'z': 15, 'ɮ': 15, 'ʃ': 15, 'ʒ': 15, 'ʂ': 15, 'ʐ': 15, 'ɕ': 15, 'ʑ': 15, 'ʝ': 15, 'ɕ': 15, 'ʑ': 15, 's': 15,\n",
        "    'ɸ': 16, 'β': 16, 'f': 16, 'v': 16, 'ⱱ̟': 16, 'ⱱ': 16,\n",
        "    'θ̼': 17, 'ð̼': 17, 'θ': 17, 'ð': 17, 'θ̠': 17, 'ð̠': 17, 'ɹ̠̊˔': 17, 'ɹ̠˔': 17, 'ɻ̊˔': 17, 'ɻ˔': 17, 'th': 17,\n",
        "    'ɺ̥': 18, 'ɺ': 18, 'ɾ̼': 18, 'ɾ̥': 18, 'ɾ': 18, 'ɽ̊': 18, 'ɽ': 18, 'ɹ': 18, 'ɻ': 18, 'ʁ': 18,\n",
        "    'ç': 19, 'ʕ': 19, 'h': 19, 'ɦ': 19,\n",
        "    'ɰ': 20, 'w': 20,\n",
        "    'r': 21, 'r̥': 21, 'ɽ̊r̥': 21, 'ɽr': 21, 'ʀ̥': 21, 'ʀ': 21,\n",
        "    'ʜ': 22, 'ʢ': 22,\n",
        "    'ɬ': 23, 'ꞎ': 23, 'ʎ̝': 23, 'ʟ̝': 23, 'l': 23, 'ɭ': 23, 'ʎ': 23, 'ʟ': 23, 'ʟ̠': 23, 'ʎ̆': 23, 'ʟ̆': 23\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Custom JSON encoder to handle NumPy arrays\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n",
        "\n",
        "# Function to transcribe IPA string to numerical values using ipa_mapping\n",
        "def transcribe_ipa(ipa_string):\n",
        "    ipa_transcription = []\n",
        "    for char in ipa_string:\n",
        "        if char in ipa_mapping:\n",
        "            ipa_transcription.append(ipa_mapping[char])\n",
        "    return np.array(ipa_transcription)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/greeeeek.json'\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/greekfinal.json'\n",
        "max_length = 18  # Maximum length for numerical representations\n",
        "\n",
        "# Open input and output files\n",
        "with open(input_file, 'r') as file_in, open(output_file, 'w', encoding='utf-8') as file_out:\n",
        "    data = json.load(file_in)\n",
        "    rows = data['rows']\n",
        "    for row in rows:\n",
        "        word = row['word']\n",
        "        pronunciation = row['pronunciation']\n",
        "        embedding = row['embedding']\n",
        "        \n",
        "        # Convert pronunciation to numerical representation\n",
        "        transcription = transcribe_ipa(pronunciation)\n",
        "\n",
        "        # Truncate the transcription if the length exceeds the maximum\n",
        "        if len(transcription) > max_length:\n",
        "            transcription = transcription[:max_length]\n",
        "\n",
        "        # Pad the transcription values with zeros to match the maximum length\n",
        "        padded_transcription = np.pad(transcription, (0, max_length - len(transcription)))\n",
        "\n",
        "        # Replace the incorrect embedding with the correct embedding\n",
        "        row['embedding'] = padded_transcription.tolist()\n",
        "\n",
        "    # Write the updated data to the output file with formatted indentation and non-ASCII characters preserved\n",
        "    json.dump(data, file_out, cls=NumpyEncoder, indent=4, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/updated_turkish.json'\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/tk.json'\n",
        "code = 'tk_'  # Code to add in front of the number in \"id\"\n",
        "\n",
        "# Open input and output files\n",
        "with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8') as file_out:\n",
        "    data = json.load(file_in)\n",
        "    rows = data['rows']\n",
        "    for row in rows:\n",
        "        entity_id = row['id']\n",
        "        modified_id = f'{code}{entity_id}'\n",
        "        row['id'] = modified_id\n",
        "\n",
        "    # Write the updated data to the output file with formatted indentation and non-ASCII characters preserved\n",
        "    json.dump(data, file_out, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "kEnt0-NeaF5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "\n",
        "def split_json(input_file, output_prefix, num_files):\n",
        "    with open(input_file, 'r') as input_file:\n",
        "        data = json.load(input_file)\n",
        "        total_rows = len(data[\"rows\"])\n",
        "        rows_per_file = math.ceil(total_rows / num_files)\n",
        "\n",
        "        for i in range(num_files):\n",
        "            start_index = i * rows_per_file\n",
        "            end_index = min((i + 1) * rows_per_file, total_rows)\n",
        "            subset_data = {\n",
        "                \"rows\": data[\"rows\"][start_index:end_index]\n",
        "            }\n",
        "            output_file = f\"{output_prefix}_{i}.json\"\n",
        "            with open(output_file, 'w') as output_file:\n",
        "                json.dump(subset_data, output_file, indent=4)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/boyah/german_milvus_formatted.json'\n",
        "output_prefix = 'german'\n",
        "num_files = 3\n",
        "\n",
        "# Split the JSON file into multiple files\n",
        "split_json(input_file_path, output_prefix, num_files)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sZ0MER1Qd6QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def fix_json_field_names(json_data):\n",
        "    fixed_data = []\n",
        "    for row in json_data[\"rows\"]:\n",
        "        fixed_row = {}\n",
        "        for key, value in row.items():\n",
        "            fixed_key = key.strip()  # Remove leading/trailing spaces from the field name\n",
        "            fixed_row[fixed_key] = value\n",
        "        fixed_data.append(fixed_row)\n",
        "    json_data[\"rows\"] = fixed_data\n",
        "    return json_data\n",
        "\n",
        "# Read the JSON file with UTF-8 encoding\n",
        "with open(\"/content/drive/MyDrive/boyah/arab2_2.json\", encoding=\"utf-8\") as file:\n",
        "    json_content = json.load(file)\n",
        "\n",
        "# Fix the field names\n",
        "fixed_json = fix_json_field_names(json_content)\n",
        "\n",
        "# Write the fixed JSON to a new file with UTF-8 encoding\n",
        "with open(\"arab6.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(fixed_json, file, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "uBSqjZr-iDAj",
        "outputId": "a9b90c80-6245-4fbe-bb9b-8b72e652a2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-819552cfe6df>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Write the fixed JSON to a new file with UTF-8 encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arab6.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install epitran"
      ],
      "metadata": {
        "id": "BXZPIqP89N4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4b89a9-fe98-401f-90af-7369bd632ff6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.24-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (67.7.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2022.10.31)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.20.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marisa-trie (from epitran)\n",
            "  Downloading marisa_trie-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.27.1)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.22.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.6.2)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.4)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=bb4da996d5702864db7874bcdf926f1e0381d0dfc3044267bdf7efb8ce28b5ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, munkres, panphon, marisa-trie, epitran\n",
            "Successfully installed epitran-1.24 marisa-trie-0.8.0 munkres-1.1.4 panphon-0.20.0 unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from g2p_en import G2p\n",
        "\n",
        "def transcribe_greek_words(json_file):\n",
        "    g2p_transcriber = G2p()\n",
        "\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for entry in data['rows']:\n",
        "        greek_word = entry['word']\n",
        "        transcription = g2p_transcriber(greek_word)\n",
        "        entry['pronunciation'] = ' '.join(transcription)\n",
        "\n",
        "    with open(json_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False)\n",
        "\n",
        "    print(\"Transcription completed and JSON file updated.\")\n",
        "\n",
        "# Usage example\n",
        "json_file_path = '/content/drive/MyDrive/Colab Notebooks/gr.json'\n",
        "transcribe_greek_words(json_file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZFMMU4SQEYz",
        "outputId": "36870485-5bcc-413d-fc7e-7783cb44c61d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription completed and JSON file updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from greek_accentuation.characters import *\n",
        "\n",
        "def transliterate_greek_words(json_file):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for entry in data['rows']:\n",
        "        greek_word = entry['word']\n",
        "        transliteration = ''.join([GREEK_TO_LATIN.get(c, c) for c in greek_word])\n",
        "        entry['pronunciation'] = transliteration\n",
        "\n",
        "    with open(json_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False)\n",
        "\n",
        "    print(\"Transliteration completed and JSON file updated.\")\n",
        "\n",
        "# Usage example\n",
        "json_file_path = '/content/drive/MyDrive/Colab Notebooks/gr.json'\n",
        "transliterate_greek_words(json_file_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Ozixcpj3lDnV",
        "outputId": "63479471-b270-4d26-e90e-14df02a57f61"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-b8672d105b1c>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Usage example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mjson_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/gr.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtransliterate_greek_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-b8672d105b1c>\u001b[0m in \u001b[0;36mtransliterate_greek_words\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rows'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgreek_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtransliteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGREEK_TO_LATIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgreek_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pronunciation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransliteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-b8672d105b1c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rows'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgreek_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtransliteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGREEK_TO_LATIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgreek_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pronunciation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransliteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GREEK_TO_LATIN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install g2p_en\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvAvQUXelE5k",
        "outputId": "cb49ca9c-64d6-42d6-e1da-058baf688858"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting g2p_en\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (1.22.4)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (6.0.4)\n",
            "Collecting distance>=0.1.3 (from g2p_en)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p_en) (1.10.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (8.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p_en) (4.5.0)\n",
            "Building wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=21d4ae2655388b2719b6ff74fef119a20eeb0c1afa152fe07e39e0fb1d825fc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built distance\n",
            "Installing collected packages: distance, g2p_en\n",
            "Successfully installed distance-0.1.3 g2p_en-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pykakasi\n",
        "\n",
        "def transliterate_japanese_words(word_list_file, output_file):\n",
        "    kakasi = pykakasi.kakasi()\n",
        "    kakasi.setMode(\"J\", \"H\")\n",
        "    converter = kakasi.getConverter()\n",
        "\n",
        "    rows = []\n",
        "    with open(word_list_file, 'r', encoding='utf-8') as file:\n",
        "        word_list = file.read().splitlines()\n",
        "\n",
        "    for i, word in enumerate(word_list):\n",
        "        entry = {\n",
        "            \"id\": f\"jp_{i}\",\n",
        "            \"word\": word,\n",
        "            \"embedding\": [0] * 18,\n",
        "            \"pronunciation\": converter.do(word)\n",
        "        }\n",
        "        rows.append(entry)\n",
        "\n",
        "    data = {\"rows\": rows}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"Transliteration completed and JSON file created.\")\n",
        "\n",
        "# Usage example\n",
        "word_list_file = '/content/drive/MyDrive/japanese.txt'\n",
        "output_file = 'japanese.json'\n",
        "transliterate_japanese_words(word_list_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjPvXIFqVUqd",
        "outputId": "5eb2d5dd-d260-4935-aaf2-706a92a929cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-9ad6078f73b2>:6: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
            "  kakasi.setMode(\"J\", \"H\")\n",
            "<ipython-input-20-9ad6078f73b2>:7: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
            "  converter = kakasi.getConverter()\n",
            "<ipython-input-20-9ad6078f73b2>:18: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
            "  \"pronunciation\": converter.do(word)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transliteration completed and JSON file created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install romkan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBArTrr9VW8y",
        "outputId": "d90847d9-0036-487a-8ee7-79990e132e38"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting romkan\n",
            "  Downloading romkan-0.2.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: romkan\n",
            "  Building wheel for romkan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for romkan: filename=romkan-0.2.1-py3-none-any.whl size=9252 sha256=9a58dbd946dce96a09bcd0ca7a1a22690f4d0633d9febf06ef2e5b615cb3e664\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/2d/cc/074f978dbf64f2cf699bc32cc2e8f5a61a1e82f0d781984056\n",
            "Successfully built romkan\n",
            "Installing collected packages: romkan\n",
            "Successfully installed romkan-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from romkan import to_roma\n",
        "\n",
        "def transliterate_japanese_words(word_list_file, output_file):\n",
        "    rows = []\n",
        "    with open(word_list_file, 'r', encoding='utf-8') as file:\n",
        "        word_list = file.read().splitlines()\n",
        "\n",
        "    for i, word in enumerate(word_list):\n",
        "        pronunciation = to_roma(word)\n",
        "        entry = {\n",
        "            \"id\": f\"jp_{i}\",\n",
        "            \"word\": word,\n",
        "            \"embedding\": [0] * 18,\n",
        "            \"pronunciation\": pronunciation\n",
        "        }\n",
        "        rows.append(entry)\n",
        "\n",
        "    data = {\"rows\": rows}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"Transliteration completed and JSON file created.\")\n",
        "\n",
        "# Usage example\n",
        "word_list_file = '/content/drive/MyDrive/japanese.txt'\n",
        "output_file = 'japanese.json'\n",
        "transliterate_japanese_words(word_list_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqodvPDvXEFX",
        "outputId": "44568eb6-36af-4160-e4e0-c88a7d36efe8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transliteration completed and JSON file created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "GREEK_TO_IPA = {\n",
        "    'α': 'a',\n",
        "    'β': 'v',\n",
        "    'γ': 'ɣ',\n",
        "    'δ': 'ð',\n",
        "    'ε': 'ɛ',\n",
        "    'ζ': 'z',\n",
        "    'η': 'i',\n",
        "    'θ': 'θ',\n",
        "    'ι': 'i',\n",
        "    'κ': 'k',\n",
        "    'λ': 'l',\n",
        "    'μ': 'm',\n",
        "    'ν': 'n',\n",
        "    'ξ': 'ks',\n",
        "    'ο': 'ɔ',\n",
        "    'π': 'p',\n",
        "    'ρ': 'r',\n",
        "    'σ': 's',\n",
        "    'τ': 't',\n",
        "    'υ': 'u',\n",
        "    'φ': 'f',\n",
        "    'χ': 'x',\n",
        "    'ψ': 'ps',\n",
        "    'ω': 'ɔ',\n",
        "    'ά': 'a',\n",
        "    'έ': 'ɛ',\n",
        "    'ή': 'i',\n",
        "    'ί': 'i',\n",
        "    'ό': 'ɔ',\n",
        "    'ύ': 'u',\n",
        "    'ώ': 'ɔ',\n",
        "    'ς': 's',\n",
        "    'ϊ': 'i',\n",
        "    'ΐ': 'i',\n",
        "    'ϋ': 'u',\n",
        "    'ΰ': 'u',\n",
        "    'Α': 'a',\n",
        "    'Β': 'v',\n",
        "    'Γ': 'ɣ',\n",
        "    'Δ': 'ð',\n",
        "    'Ε': 'ɛ',\n",
        "    'Ζ': 'z',\n",
        "    'Η': 'i',\n",
        "    'Θ': 'θ',\n",
        "    'Ι': 'i',\n",
        "    'Κ': 'k',\n",
        "    'Λ': 'l',\n",
        "    'Μ': 'm',\n",
        "    'Ν': 'n',\n",
        "    'Ξ': 'ks',\n",
        "    'Ο': 'ɔ',\n",
        "    'Π': 'p',\n",
        "    'Ρ': 'r',\n",
        "    'Σ': 's',\n",
        "    'Τ': 't',\n",
        "    'Υ': 'u',\n",
        "    'Φ': 'f',\n",
        "    'Χ': 'x',\n",
        "    'Ψ': 'ps',\n",
        "    'Ω': 'ɔ',\n",
        "    'Ά': 'a',\n",
        "    'Έ': 'ɛ',\n",
        "    'Ή': 'i',\n",
        "    'Ί': 'i',\n",
        "    'Ό': 'ɔ',\n",
        "    'Ύ': 'u',\n",
        "    'Ώ': 'ɔ',\n",
        "    'Ϊ': 'i',\n",
        "    'Ϋ': 'u'\n",
        "}\n",
        "\n",
        "\n",
        "def transcribe_greek_words(json_file):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for entry in data['rows']:\n",
        "        greek_word = entry['word']\n",
        "        ipa_transcription = ''.join([GREEK_TO_IPA.get(c, c) for c in greek_word])\n",
        "        entry['pronunciation'] = ipa_transcription\n",
        "\n",
        "    with open(json_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False)\n",
        "\n",
        "    print(\"Transcription completed and JSON file updated.\")\n",
        "\n",
        "# Usage example\n",
        "json_file_path = '/content/drive/MyDrive/Colab Notebooks/greeeeek.json'\n",
        "transcribe_greek_words(json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGZff8xZcmiZ",
        "outputId": "1b50cd79-4c83-4da0-96ed-9cf31c82dadc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription completed and JSON file updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "HEBREW_TO_IPA = {\n",
        "    'א': '',\n",
        "    'ב': 'v',\n",
        "    'ג': 'ɡ',\n",
        "    'ד': 'd',\n",
        "    'ה': 'h',\n",
        "    'ו': 'v',\n",
        "    'ז': 'z',\n",
        "    'ח': 'χ',\n",
        "    'ט': 't',\n",
        "    'י': 'j',\n",
        "    'כ': 'k',\n",
        "    'ך': 'k',\n",
        "    'ל': 'l',\n",
        "    'מ': 'm',\n",
        "    'ם': 'm',\n",
        "    'נ': 'n',\n",
        "    'ן': 'n',\n",
        "    'ס': 's',\n",
        "    'ע': '',\n",
        "    'פ': 'p',\n",
        "    'ף': 'p',\n",
        "    'צ': 'ts',\n",
        "    'ץ': 'ts',\n",
        "    'ק': 'k',\n",
        "    'ר': 'ʁ',\n",
        "    'ש': 'ʃ',\n",
        "    'ת': 't',\n",
        "    'װ': 'v',\n",
        "    'ױ': 'ɔj',\n",
        "    'ײ': 'ej',\n",
        "    'ְ': '',\n",
        "    'ֱ': 'ɛ',\n",
        "    'ֲ': 'a',\n",
        "    'ֳ': 'ɔ',\n",
        "    'ִ': 'i',\n",
        "    'ֵ': 'e',\n",
        "    'ֶ': 'ɛ',\n",
        "    'ַ': 'a',\n",
        "    'ָ': 'a',\n",
        "    'ֹ': 'o',\n",
        "    'ֺ': 'o',\n",
        "    'ֻ': 'u',\n",
        "    'ּ': '',\n",
        "    'ֽ': '',\n",
        "    '־': '',\n",
        "    'ֿ': '',\n",
        "    '׀': '',\n",
        "    'ׁ': '',\n",
        "    'ׂ': '',\n",
        "    '׃': '',\n",
        "    'ׄ': '',\n",
        "    'ׅ': '',\n",
        "    'ׇ': ''\n",
        "}\n",
        "\n",
        "\n",
        "def convert_hebrew_to_ipa(hebrew_word):\n",
        "    ipa_word = ''.join([HEBREW_TO_IPA.get(c, c) for c in hebrew_word])\n",
        "    return ipa_word\n",
        "\n",
        "def process_hebrew_files(input_file1, input_file2, input_file3, output_file):\n",
        "    data = {'rows': []}\n",
        "    seen_words = set()\n",
        "\n",
        "    def add_word(word):\n",
        "        if word not in seen_words:\n",
        "            data['rows'].append({\n",
        "                'id': f'he_{len(data[\"rows\"])}',\n",
        "                'word': word,\n",
        "                'embedding': [0.0] * 18,\n",
        "                'pronunciation': convert_hebrew_to_ipa(word)\n",
        "            })\n",
        "            seen_words.add(word)\n",
        "\n",
        "    with open(input_file1, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            word = line.strip()\n",
        "            add_word(word)\n",
        "\n",
        "    with open(input_file2, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            word = line.strip()\n",
        "            add_word(word)\n",
        "\n",
        "    with open(input_file3, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            word = line.strip()\n",
        "            add_word(word)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Usage example\n",
        "input_file1 = '/content/drive/MyDrive/words.txt'\n",
        "input_file2 = '/content/drive/MyDrive/bible.txt'\n",
        "input_file3 = '/content/drive/MyDrive/all_with_fatverb.txt'\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/he.json'\n",
        "process_hebrew_files(input_file1, input_file2, input_file3, output_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "MuyRZOu9iCvU"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}